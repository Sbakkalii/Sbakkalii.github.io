---
layout: archive
title: "Scientific Publications"
permalink: /publications/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}
## <i> **Activités de Recherche** </i>

Mes travaux de recherche ont été réalisés au sein du laboratoire L3i (dans l’équipe IC), du département informatique de La Rochelle Université entre décembre 2019 et décembre 2022. Mes travaux s’intègrent principalement dans le domaine de l’analyse et la compréhension des documents administratifs. Ces documents peuvent être des documents papier et/ou numériques produites par les grandes institutions publiques ou privées, intégrant différents types de contenus très hétérogènes (structurées et non-structurées). En effet, ces contenus se présentent souvent sous diverses formes, sous forme de graphiques dans des rapports techniques, de diagrammes dans des articles scientifiques et de conceptions graphiques dans des bulletins. Afin de prendre des décisions sur des sujets d'intérêt tels que la science, l’industrie, la santé, etc., l’être humain peut traiter efficacement les informations visuelles et textuelles contenues dans ces documents. Toutefois, comprendre et analyser manuellement de grandes quantités de données à partir de documents prend généralement du temps et coûte cher. 

En général, les données de document sont souvent présentées dans des mises en page (layout) complexes en raison des différentes manières d'organiser chaque document. Contrairement aux images générales de scènes naturelles, les documents sont très difficiles compte tenu de leurs propriétés structurelles visuelles et de leur contenu textuel hétérogène. Dans ces conditions, le développement d'outils informatiques capables de comprendre et d'extraire automatiquement des informations structurées précises à partir d'une grande variété de documents reste crucial, d'une manière qui conduit à effectuer d'importantes applications administratives et/ou commerciales. 

Il existe aujourd'hui plusieurs applications utilisées pour comprendre automatiquement les données des documents administratifs et commerciaux telles que : la classification des documents, la récupération de documents basée sur le contenu et la classification de documents à quelques exemples. Par conséquent, la clé de la compréhension automatisée des documents réside dans l'intégration efficace des signaux provenant de plusieurs modalités de données. Étant donné que les documents sont nativement multimodaux, il est important de tirer parti des informations multi-modales du langage et de la vision. Contrairement à d'autres formats de données tels que les images ou leur texte brut extrait à partir d’une reconnaissance optique de caractères (OCR), les documents combinent des informations visuelles et linguistiques, complétées par la mise en page du document. En outre, d'un point de vue pratique, de nombreuses tâches liées à la compréhension des documents sont rares. Un modèle qui peut apprendre à partir de documents non étiquetés (c.-à-d. un pré-entraînement), effectuer un ajustement du modèle pour des applications de documents spécifiques est plus préféré que celui qui nécessite des données d’entraînement entièrement annotées (c.-à-d. entraînés dans un mode d'apprentissage entièrement supervisé).

Le propre de mes travaux de recherche actuelle s'inscrit dans le cadre de la compréhension et l’analyse des images de documents administratifs (courriels, factures, publicités, articles, rapports, etc.), qui a été largement adoptée dans diverses applications de traitement d'images de documents. Mes travaux de recherche se concentrent principalement sur les interactions inter-modales entre les informations visuelles et textuelles dans les images de documents, visant la conception d'un environnement d'apprentissage efficace. En effet, le processus de conception de tels systèmes implique l'étude des avantages des interactions inter-modales dans l'apprentissage multi-modal. De tels systèmes encouragent l'apprentissage inter-modal entre les caractéristiques visuelles et textuelles des modalités visuelles et langagières afin d'améliorer leur distribution dans l'espace de représentation commun. Encore, les modèles développés sont le résultat d'un processus itératif d'analyse et de synthèse entre les théories existantes et nos études réalisées. Le propre de ma recherche actuelle part alors du fait d’étudier l'apprentissage inter-modal pour la compréhension contextualisée sur les composantes du document à travers le langage et la vision. L'idée principale est de tirer parti des informations multi-modales des images de documents dans un espace sémantique commun. Le principe consiste à extraire automatiquement des informations du contenu présenté dans les systèmes d'information (scan des documents, informations structurées et non structurées). Ensuite, comprendre les interactions entre données visuelles et textuelles, réorganiser 

## <i> **Rayonnement scientifique** </i>

You can also find my articles on <u><a href="https://scholar.google.com/citations?user=gO_Q48IAAAAJ&hl=fr">my Google Scholar profile</a>.</u>

### <i> **International Journals** </i>

[J1] **Bakkali, S.**, Biswas, S., Ming, Z., Coustaty, M., Rusiñol, M., Terrades, O. R., & Lladós, J. Transferdoc: A Self-Supervised Transferable Document Representation Learning Model Unifying Vision and Language. Available at SSRN 4545314. **(SJR rang Q1, IF 8.518)**

[Access the paper here](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4545314)

[J2] **Bakkali, S.**, Ming, Z., Coustaty, M., Rusiñol, M., & Terrades, O. R. (2023). VLCDoC: Vision-Language contrastive pre-training model for cross-Modal document classification. Pattern Recognition, 139, 109419. **(SJR rang Q1, IF 8.518)**

[Access the paper here](https://arxiv.org/pdf/2205.12029.pdf)

[J3] **Bakkali, S.**, Ming, Z., Coustaty, M., & Rusiñol, M. (2021). EAML: ensemble self-attention-based mutual learning network for document image classification. International Journal on Document Analysis and Recognition (IJDAR), 24(3), 251-268. **(SJR rang Q1, IF 3.870)**

[Access the paper here](https://arxiv.org/pdf/2305.06923.pdf)

#### <i> **International Conferences & Workshops** </i>

[C1] **Bakkali, S.**, Ming, Z., Coustaty, M., & Rusiñol, M. (2020). Visual and textual deep feature fusion for document image classification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops (pp. 562-563).

[Access the paper here](https://openaccess.thecvf.com/content_CVPRW_2020/papers/w34/Bakkali_Visual_and_Textual_Deep_Feature_Fusion_for_Document_Image_Classification_CVPRW_2020_paper.pdf)

[C2] **Bakkali, S.**, Ming, Z., Coustaty, M., & Rusiñol, M. (2020, October). Cross-modal deep networks for document image classification. In 2020 IEEE International Conference on Image Processing (ICIP) (pp. 2556-2560). IEEE. **(Core rang B)**

[Access the paper here](https://www.researchgate.net/profile/Zuheng-Ming/publication/345998752_Cross-Modal_Deep_Networks_For_Document_Image_Classification/links/62c6f92b00d0b451103de6c1/Cross-Modal-Deep-Networks-For-Document-Image-Classification.pdf)

[C3] **Bakkali, S.**, Luqman, M. M., Ming, Z., & Burie, J. C. (2019, September). Face detection in camera captured images of identity documents under challenging conditions. In 2019 International Conference on Document Analysis and Recognition Workshops (ICDARW) (Vol. 4, pp. 55-60). IEEE.

[Access the paper here](https://arxiv.org/pdf/1911.03567.pdf)

#### <i> [**Thesis**] </i>

**Bakkali, S.**. Multimodal Document Understanding with Unified Vision and Language Cross-Modal Learning. Laboratoire L3i de La Rochelle Université, Thesis, 2022.



